Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
2000,0.0,0.5415000319480896,0.5667500495910645,0.5414999723434448,0.5667499899864197,0.5034390242262634,0.5038530087193397,0.503439081248803,0.5038529878951017
2000,125,0.2569999694824219,0.2668928603331248,0.25699999928474426,0.2670833269755046,0.2595358071257232,0.2687629857549917,0.2595436351871859,0.26877119034935026
2000,250,0.22298294305801392,0.27159125606218976,0.22300000488758087,0.27166666587193805,0.22475138089365762,0.2723123951742578,0.2247818884472286,0.27234827180026755
2000,500,0.17948757112026215,0.2302656869093577,0.17949999868869781,0.23033333321412405,0.1768494291948457,0.23453053199554688,0.17692860848160302,0.23459915500838271
2000,1000,0.12331942468881607,0.23082130153973898,0.12349999696016312,0.23100000123182932,0.12315319685158058,0.23047149344342074,0.1233667723798917,0.23087584003943878
2000,2000,0.041800759732723236,0.23084194461504617,0.042500000447034836,0.23233332733313242,0.04360306266911365,0.22975010195578724,0.044252351956224976,0.23094477812942385
2000,4000,0.005589299835264683,0.24603274961312613,0.006000000052154064,0.2513333360354106,0.009227692123268752,0.24293973054169543,0.010490030813107988,0.24851215680215702
2000,8000,0.0021677399054169655,0.21664459009965262,0.003000000026077032,0.2330833375453949,0.007174399816160658,0.21991134559580655,0.008941429305988275,0.23396324087470724
2000,16000,0.003268232336267829,0.21236156423886618,0.0024999999441206455,0.2329166680574417,0.004359983778535988,0.21479323808221443,0.004159293207039898,0.2354283424945787

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN2L
  - Number of hidden layers: 2
  - Width of hidden layers: 1000
  - Dataset type: cifar10
  - Random labels: False
  - Training set size: 2000
  - Test set size: 9000
  - Minimum epochs: 4000
  - Number of Batches: 1
  - Beta values: [0.0, 125, 250, 500, 1000, 2000, 4000, 8000, 16000]
  - Learning rate (a0): 0.01
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
"
