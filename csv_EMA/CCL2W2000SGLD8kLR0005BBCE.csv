Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
8000,0.0,0.6100000143051147,0.5050000548362732,0.6100000143051147,0.5049999952316284,0.49399110303629784,0.4941974977367167,0.49399108031914735,0.49419749962196363
8000,500,0.23622005246579647,0.24298530519008638,0.23687499919906257,0.24344444308016036,0.23063897680412493,0.2314154867483331,0.23074794485490846,0.2317113148253216
8000,1000,0.21104585900902748,0.20743506302436193,0.21150000048801304,0.20844444235165913,0.20699596944806592,0.2226390876379513,0.20743157534016812,0.22311943069457787
8000,2000,0.17306459667161106,0.1965838395886951,0.17537500001490117,0.19977777666515775,0.16860660131282862,0.1935027895173936,0.17005606584109903,0.19489720181818135
8000,4000,0.11528260577470065,0.1759648182325893,0.12962499978020786,0.18988888826635147,0.12423271004733437,0.18209733613969456,0.12921883225175568,0.18860539527877118
8000,8000,0.06702747305389493,0.16691164639261033,0.08475000017788262,0.19155555566151936,0.06837858612613537,0.17656028249868033,0.0786783659023729,0.19177492040564365
8000,16000,0.03060044760350138,0.18621143698692322,0.04737499970942736,0.24277777506245507,0.03002818557769524,0.16601251803816094,0.04419707957160007,0.20163529122723067
8000,32000,0.01755524105974473,0.12549601048231124,0.021624999563209714,0.18577777809566923,0.012176317275007568,0.13923269287326562,0.015833525327037894,0.18921391254816305
8000,64000,0.0076995022274786605,0.12173374113109377,0.004624999896623194,0.17377777894337973,0.006434793819052966,0.1314132213638892,0.004492667504162639,0.17702321077395036

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN2L
  - Number of hidden layers: 2
  - Width of hidden layers: 2000
  - Dataset type: cifar10
  - Random labels: False
  - Training set size: 8000
  - Test set size: 9000
  - Minimum epochs: 4000
  - Number of Batches: 80
  - Beta values: [0.0, 500, 1000, 2000, 4000, 8000, 16000, 32000, 64000]
  - Learning rate (a0): 0.005
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
 -  Gradient norm: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
"
