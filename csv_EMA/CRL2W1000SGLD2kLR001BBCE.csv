Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
2000,0.0,0.4955000214278698,0.502666691939036,0.4954999998211861,0.5026666614744398,0.5004075860534256,0.5006228351787076,0.50040765146659,0.5006229615181711
2000,125,0.5105000324547291,0.5000327196386125,0.5105000011622906,0.5001111080249151,0.5134044270811458,0.5010313234488551,0.5134043949827531,0.5010314327786218
2000,250,0.48800002187490465,0.49966668751504684,0.48799999579787257,0.49966666234864127,0.4871156090311931,0.49988763681187137,0.48712462549541335,0.49989496824692453
2000,500,0.51100003272295,0.5002286430862215,0.5109999969601631,0.5003333297040727,0.48959654277537523,0.5002141492595429,0.489627567244757,0.5002207897548335
2000,1000,0.4167297787964344,0.4951408925983641,0.4175000011920929,0.4973333285914527,0.4504192846986644,0.4988879181810043,0.4511495717697337,0.4995609135611503
2000,2000,0.2585186991840601,0.4760831071270837,0.2725000001490116,0.49722222089767454,0.3182250148837548,0.48752233102932935,0.32404561829369044,0.4948414064548259
2000,4000,0.08622060791531112,0.4666127324104309,0.10299999853596091,0.49599999785423277,0.11005590128976885,0.479748939891888,0.1227062456560828,0.4981427460126761
2000,8000,0.029820697085324355,0.4568316535817252,0.04249999914318323,0.4894444422589408,0.031144197086115204,0.47097082677786245,0.03863805936199747,0.49656701086233185
2000,16000,0.008699369721580297,0.372505756550365,0.005999999865889549,0.4913333323266771,0.020786277613982048,0.42449015792365463,0.028729243270501602,0.4968650901814507

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN2L
  - Number of hidden layers: 2
  - Width of hidden layers: 1000
  - Dataset type: cifar10
  - Random labels: True
  - Training set size: 2000
  - Test set size: 9000
  - Minimum epochs: 4000
  - Number of Batches: 40
  - Beta values: [0.0, 125, 250, 500, 1000, 2000, 4000, 8000, 16000]
  - Learning rate (a0): 0.01
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
"
