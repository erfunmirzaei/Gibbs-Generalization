Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
2000,0.0,0.5625000342726707,0.5673967867481465,0.5625000037252903,0.5673967709346693,0.5026052981677622,0.5022068176982436,0.5026051090315097,0.5022068641354438
2000,125,0.20779809448868036,0.23198713833580212,0.20849999943748115,0.23264594132802924,0.23180446225026857,0.2817263865116473,0.2320558817299017,0.28207324059923156
2000,250,0.16528109908103944,0.19804579962273033,0.16850000005215407,0.20065733281021214,0.16879839277205122,0.2046682393875458,0.17038250061707175,0.20613083207721022
2000,500,0.10591186783276499,0.1773913865338783,0.1089999994263053,0.18116990993825757,0.13431578958501741,0.2116345386984397,0.1372646058310277,0.21394411115265566
2000,1000,0.06426007766276598,0.13421620337330564,0.06799999941140414,0.13953725708534523,0.0553943852360609,0.12952478848164842,0.05729756545531125,0.13324169985447679
2000,2000,0.018991077327887317,0.10557258744933168,0.020999999530613423,0.1146725200268687,0.028753653456363267,0.11339602751559696,0.03303170491904929,0.12113188381657902
2000,4000,0.0037380010603442317,0.10519316993957879,0.0054999998770654205,0.12072852409767862,0.004687488986085499,0.10469810558009908,0.007256752560445721,0.11570645885247978
2000,8000,0.0021496579462109366,0.08275901855026581,0.0024999999441206455,0.10418130048759738,0.0021536373272609236,0.09113936708540699,0.0019001475563258524,0.11209719882304581
2000,16000,0.0018120277192792856,0.07299778365282988,0.0,0.10159705702823643,0.0016495854516604392,0.07613370614029384,0.000335655207054876,0.1015750087483551

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN2L
  - Number of hidden layers: 2
  - Width of hidden layers: 1000
  - Dataset type: mnist
  - Random labels: False
  - Training set size: 2000
  - Test set size: 9786
  - Minimum epochs: 4000
  - Number of Batches: 40
  - Beta values: [0.0, 125, 250, 500, 1000, 2000, 4000, 8000, 16000]
  - Learning rate (a0): 0.005
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
"
