Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
8000,0.0,0.4599999487400055,0.5199999809265137,0.46000000834465027,0.5199999809265137,0.49287131920528665,0.49854656566126543,0.492871321062066,0.49854654341599486
8000,500,0.17444722035434096,0.17937698245656733,0.17537500101607292,0.17996269570929663,0.20453233961322212,0.289170624109984,0.20472824381121538,0.2893754045815157
8000,1000,0.12484728880226612,0.1285357070820672,0.12712499951012432,0.13042681633817907,0.13816960372323506,0.15115690318976496,0.1385792463971377,0.15158009102538963
8000,2000,0.07068035474512727,0.09802205936641109,0.07275000009685754,0.10089861730835875,0.07502961565901864,0.09367155808210663,0.07617012834034015,0.09480007395648349
8000,4000,0.0358239853347186,0.06944715383709693,0.04224999975413084,0.0778933506535024,0.04345334035910396,0.08023016401645144,0.04650138035283753,0.08480523788782218
8000,8000,0.015288342699295753,0.05347518850954212,0.02024999966379255,0.06232938349094926,0.01762734963016756,0.06080298676548719,0.02170773111313674,0.06828370512708766
8000,16000,0.004620894388790475,0.05103150530889326,0.006749999849125743,0.06501316628894027,0.005447256424181616,0.0521002159388457,0.007954664061305308,0.06183895528766009
8000,32000,0.003459977303191408,0.04436707023379146,0.0037499999161809683,0.05752578440445418,0.0033066336436465434,0.0468184096472627,0.003824390283157111,0.05947643076807583
8000,64000,0.0026231102852761977,0.03783745854636844,0.0014999999664723873,0.052949308702836234,0.002332387375436032,0.04041695900527128,0.001598019369759263,0.054935203738912815

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN3L
  - Number of hidden layers: 3
  - Width of hidden layers: 500
  - Dataset type: mnist
  - Random labels: False
  - Training set size: 8000
  - Test set size: 9786
  - Minimum epochs: 4000
  - Number of Batches: 80
  - Beta values: [0.0, 500, 1000, 2000, 4000, 8000, 16000, 32000, 64000]
  - Learning rate (a0): 0.01
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
 -  Gradient norm: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
"
