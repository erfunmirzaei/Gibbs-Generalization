Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
2000,0.0,0.5600000023841858,0.4599999785423279,0.5600000023841858,0.46000000834465027,0.5032434151714477,0.49603435563418763,0.5032433983384659,0.49603435962265197
2000,125,0.1286240890622139,0.14651807302571074,0.13149999976158142,0.14860227867504772,0.16046417717683972,0.23182543599392302,0.16125036305143156,0.23292095365656348
2000,250,0.07385783134959638,0.10468989046176477,0.07649999903514981,0.10753203614861989,0.10905848362314668,0.1156419011815755,0.11059864236201607,0.11730803388527437
2000,500,0.04807669735091622,0.08046208131982356,0.05099999913945794,0.08640958726101992,0.04566135989119769,0.07544363784670857,0.04951662779264212,0.07987968908432509
2000,1000,0.026195847070130184,0.05299679638475788,0.035499999206513165,0.06588277158004288,0.025644972918692922,0.058586876476144814,0.03230752774837921,0.06838338592948541
2000,2000,0.014047968972590752,0.05079170037060976,0.018499999586492778,0.07321309915990853,0.013930836281616446,0.05613457352867804,0.02043178971294443,0.07232560109066132
2000,4000,0.0101992700539995,0.03355496055541598,0.01649999972432852,0.04953962969308605,0.006780178533517804,0.035450308287376386,0.00788396847567343,0.05248631668816296
2000,8000,0.007391077946522273,0.03449752395592478,0.009499999787658453,0.05209065013926248,0.00591689958359238,0.03651191020528841,0.006215451163924021,0.05326457551307793
2000,16000,0.00484409942291677,0.0337956146698217,0.0064999998547136785,0.0479069763262357,0.004636085251539217,0.0308135050891861,0.0048280547627242085,0.04403011760153248

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: LeNet5
  - Number of hidden layers: L
  - Width of hidden layers: 2000
  - Dataset type: mnist
  - Random labels: False
  - Training set size: 2000
  - Test set size: 9786
  - Minimum epochs: 4000
  - Number of Batches: 40
  - Beta values: [0.0, 125, 250, 500, 1000, 2000, 4000, 8000, 16000]
  - Learning rate (a0): 0.005
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
 -  Gradient norm: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
"
