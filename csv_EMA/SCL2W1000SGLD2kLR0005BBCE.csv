Sample_size,Beta,BCE_Train,BCE_Test,0-1_Train,0-1_Test,EMA_BCE_Train,EMA_BCE_Test,EMA_0-1_Train,EMA_0-1_Test
2000,0.0,0.541500025242567,0.5686667011843787,0.5415000021457672,0.5686666631036335,0.5038326461167895,0.5044933313464224,0.5038327137554199,0.5044933147176052
2000,125,0.2645847277715802,0.2584693529539638,0.26450000088661907,0.25855555517805945,0.2876174113304246,0.30936158598431035,0.28760339031952753,0.30950312440851646
2000,250,0.24461937937885522,0.24626733793152703,0.2450000012293458,0.2468888900346226,0.25683487120013687,0.27889218158128576,0.25717746523315027,0.27926084747116076
2000,500,0.18604515101760627,0.2291199571556515,0.1869999997317791,0.22977777702940835,0.20365596637434125,0.25836854732151854,0.20464585584447928,0.25896499925752736
2000,1000,0.13953151241876185,0.22453861468368105,0.14250000016763806,0.2276666656136513,0.15258947171766513,0.21694180289726192,0.15433137950892326,0.21916680828955196
2000,2000,0.08649643819298944,0.21637388807204033,0.09449999947100877,0.22533333227038382,0.09261644475670394,0.20718092464542764,0.09701797231305469,0.21326961744603165
2000,4000,0.025457052563433537,0.20258312382631832,0.03299999935552478,0.216444443911314,0.03019734961421226,0.20967705843340448,0.03548734514010152,0.22169468745008789
2000,8000,0.009863850280817132,0.1937913615670469,0.013499999698251487,0.22700000115566785,0.008349366204339684,0.22085286616039077,0.01325538458204908,0.2496986165849235
2000,16000,0.003071916167391464,0.18736523025565677,0.0014999999664723873,0.22744444368614092,0.0032584470717555154,0.19671603337007107,0.002964379794092755,0.2331224529803094

Summary:,"The LMC has been run with the following parameters:
  - Device: cpu
  - Loss function: BBCE
  - l_max: 4.0
  - Network architecture: FCN2L
  - Number of hidden layers: 2
  - Width of hidden layers: 1000
  - Dataset type: cifar10
  - Random labels: False
  - Training set size: 2000
  - Test set size: 9000
  - Minimum epochs: 4000
  - Number of Batches: 40
  - Beta values: [0.0, 125, 250, 500, 1000, 2000, 4000, 8000, 16000]
  - Learning rate (a0): 0.005
  - Learning rate decay (b): 0.5
  - Gaussian prior sigma: 5
 -  alpha_average: 0.01
 -  alpha_stop: 0.00025
 -  eta: 0.1
 -  eps: 1e-07
"
